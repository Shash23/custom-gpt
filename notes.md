generatively pre-trained transformer

gelu over relu
- because it has a smoother curve meaning it approximates better

EX1: The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention` into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT).

